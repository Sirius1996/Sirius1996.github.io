<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Inductive Representation Learning on Large Graphs 笔记 · Hexo</title><meta name="description" content="Inductive Representation Learning on Large Graphs 笔记 - Zidi Chen"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Hexo"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="https://weibo.com/u/5519474336" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/Sirius1996" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/%E5%AD%90%E8%BF%AA-%E9%99%88-43010911a/" target="_blank" class="nav-list-link">LINKEDIN</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Inductive Representation Learning on Large Graphs 笔记</h1><div class="post-info">Mar 19, 2019</div><div class="post-content"><p>本文提出GraphSAGE，一个inductive的模型，可以使用类诸如文本属性信息的节点特征来产生之前未见节点的embedding。不再训练所有节点的每个embedding，而是训练一个函数，通过从节点的neighbour采样和手机特征来产生embedding。<br><a id="more"></a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文提出GraphSAGE，一个inductive的模型，可以使用类诸如文本属性信息的节点特征来产生之前未见节点的embedding。不再训练所有节点的每个embedding，而是训练一个函数，通过从节点的neighbour采样和手机特征来产生embedding。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>一般来说的做法是，把图的结构embedding成向量，然后输入到下一个神经网络进行节点分类。然而之前到做法，是对某一固定的图结构进行embedding，或者产生一个全新图结构的所有节点embedding。用于生成节点嵌入的大多数现有方法本质上是转换性的。</p>
<p>利用节点特征（文本属性，节点属性，节点等级）来训练一个embedding函数，将其推广到看不见的节点。过在学习算法中加入节点特征，我们可以同时学习每个节点邻域的拓扑结构以及邻域中节点特征的分布。</p>
<p>我们不是为每个节点训练不同的嵌入向量，而是训练一组聚合器函数，这些函数学习从节点的本地邻域聚合特征信息。</p>
<p><img src="/2019/03/19/Inductive-Representation-Learning-on-Large-Graphs-笔记/image-20190318153338020.png" alt="image-20190318153338020"></p>
<h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><h3 id="Embedding-生成算法"><a href="#Embedding-生成算法" class="headerlink" title="Embedding 生成算法"></a>Embedding 生成算法</h3><p>我们假设模型已经过训练，参数是固定的。其中，我们假设已经训练好K个aggregator函数。其中在模型训练过程中，不同于以往的算法学习到独特的embedding，该论文学习到的是节点的embedding function。</p>
<p><img src="/2019/03/19/Inductive-Representation-Learning-on-Large-Graphs-笔记/image-20190318161610419.png" alt="image-20190318161610419"></p>
<p>其中，$h^k_v$表示节点v在步骤k的embedding向量表示。算法1背后的直觉是在每次迭代，或搜索深度，顶点从他们的局部邻居聚合信息，并且随着这个过程的迭代，顶点会从越来越远的地方获得信息。</p>
<p>算法1描述了在各整个图上生成embedding的过程，$G = (v,\varepsilon)$以及所有顶点的特征$X_v$,$\forall v\in V$作为输入。在算法1最外层循环的每一步如下，k表示外循环（或搜索深度）的当前一步，$h^k$表示当前这步的一个顶点的表示：首先，每个顶点$v\in V$聚合了在它在中间邻居的表示,$h^{k-1}_u$,$\forall u\in N(u)$ ,∀u∈N(u)，聚合到向量$h^{k-1}_{N(v)}$中。注意，这个聚合步骤依赖于外循环前一次迭代生成的表示（比如k−1），k=0表示输入的顶点特征。聚合邻居特征向量后，GraphSAGE之后拼接了顶点当前的表示，$h^{k-1}_v$，核聚合的邻居向量一起，$h^{k-1}_{N(v)}$，拼接后的向量输入到了激活函数为σ的全连接层中，将表示变换为下一步使用的形式（$h^k_v,\forall v \in V$）。邻居表示的聚合可以通过多个聚合架构得到（在算法1中表示为<strong>AGGERGATE</strong>），我们会在3.3讨论不同的架构。</p>
<p>为了把算法1拓展到minibatch设定上，给定一组输入顶点，我们先采样猜出需要的邻居集合（到深度K），然后运行内部循环（算法1的第三行），但是不迭代所有的顶点，我们在每个深度只计算必须满足的表示。</p>
<h3 id="Learning-the-parameters-of-GraphSAGE"><a href="#Learning-the-parameters-of-GraphSAGE" class="headerlink" title="Learning the parameters of GraphSAGE"></a>Learning the parameters of GraphSAGE</h3><p>为了在半监督设定下学习一个有效的表示，我们使用基于图的损失函数来输出表示$z_u$，$\forall u \in v$，调整权重矩阵$W^k$，$\forall k \in 1,…,k$，聚合函数的参数通过随机梯度下降训练。基于图的损失函数倾向于使得梁林的顶点有相似的表示，尽管这会使相互远离的顶点的表示很不一样：</p>
<script type="math/tex; mode=display">
JG(z_u) = -log(\sigma(z^T_uz_v)) - Q \cdot \mathbb{E_{v_n~p_n(v)}}log(\sigma(-z^T_uz_{v_n}))</script><p>其中，$v$是通过定长随机游走得到的$u$旁边的共现顶点，$\sigma$是sigmoid函数，$P_n$是负采样分布，Q定义了负样本的数目。重要的是，不像之前的那些方法，输入到损失函数的表示$z_u$是从包含一个顶点局部邻居的特征生成出来的，而不是对每个顶点训练一个独一无二的embedding（通过一个embedding查询表）。</p>
<p>这个无监督设定模拟了顶点特征提供给后续机器学习应用的情况。在那些表示只在后续任务中使用的情况下，无监督损失可以被替换或改良，通过一个以任务为导向的目标函数（比如cross-entropy）。</p>
<h3 id="聚合架构"><a href="#聚合架构" class="headerlink" title="聚合架构"></a>聚合架构</h3><p>不像在N维网络（如句子、图像、3D）上的机器学习，一个顶点的邻居是无序的；因此，算法1中的聚合函数必须在以一个无序的向量上运行。理想上来说，一个聚合函数需要是对称的（也就是对它输入的全排列来说是不变的），而且还要可训练，且保持表示的能力。聚合函数的对称性确保了我们的神经网络模型可以被训练且可以应用于任意顺序的顶点邻居特征集合上。我们检验了三种聚合函数：</p>
<p><strong>Mean aggregator</strong> 第一个聚合函数是均值聚合，我们简单的取$h^{k-1}_u,\forall v \in N(v)$中的向量的element-wise值。均值聚合近似等价在transducttive GCN框架中的卷积传播规则。特别地，我们可以通过替换算法1中的4行和5行为以下内容得到GCN的inductive变形：</p>
<script type="math/tex; mode=display">
h^k_v \gets \sigma(W \cdot MEAN(h^{k-1}_v \cup h^{k-1}_u,\forall u \in N(n)))</script><p>我们称这个修改后的基于均值的聚合器是convolutional，因为它是以恶粗略的，局部话卷积的线性近似。这个卷积聚合器和我们的其他聚合器的重要不同在于它没有算法1中第5行的拼接操作——卷积聚合器没有将顶点前一层的表示$h^{k-1}_v$和聚合向量$h^k_{N(v)}$拼接起来。拼接操作可以看作一个是在不同的搜索深度或层之间的简单的skip connection的形式，它使得模型获得了巨大的提升。</p>
<p><strong>Pooling aggregator</strong> 我们检验的最后一个聚合器既是对称的，又是可训练的。在这个<em>池化</em>方法中，每个邻居的向量都是相互独立输入到全连接神经网络中的；随着这种变化，一个element-wise最大池化操作应用在邻居集合上来聚合信息：</p>
<script type="math/tex; mode=display">
AGGREGATE^{pool}_k = max(\sigma(W_{pool}h^k_{u_i}+b),\forall u_i \in N(v)),</script><p>其中，maxmax表示element-wise最大值操作，σσ是非线性激活函数。原则上，在最大池化使用之前，函数可以是任意的深度多层感知机，但是我们关注的是单个的单层结构。方法是搜到了最近的神经网络架构在学习general point sets上的启发[29]。直觉上来说，多层感知机可以看作是一组函数，这组函数为邻居集合中的每个顶点计算表示。通过对每个计算得到的特征使用最大池化操作，模型有效地捕获了邻居集合的不同方面。注意，原则上，任何对称的向量函数都可以替换maxmax操作器（比如element-wise mean）。我们发现最大池化和均值池化在测试时没有太大的差别，所以使用了最大池化完成了后续的实验。</p>
<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p>首先，邻接表的构建和采样分布在minibatch.py和neigh_samplers.py中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_adj</span><span class="params">(self)</span>:</span></span><br><span class="line">        adj = len(self.id2idx)*np.ones((len(self.id2idx)+<span class="number">1</span>, self.max_degree))</span><br><span class="line">        deg = np.zeros((len(self.id2idx),))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> nodeid <span class="keyword">in</span> self.G.nodes():</span><br><span class="line">        <span class="keyword">if</span> self.G.node[nodeid][<span class="string">'test'</span>] <span class="keyword">or</span> self.G.node[nodeid][<span class="string">'val'</span>]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        neighbors = np.array([self.id2idx[neighbor] </span><br><span class="line">            <span class="keyword">for</span> neighbor <span class="keyword">in</span> self.G.neighbors(nodeid)</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">not</span> self.G[nodeid][neighbor][<span class="string">'train_removed'</span>])])</span><br><span class="line">        deg[self.id2idx[nodeid]] = len(neighbors)</span><br><span class="line">        <span class="keyword">if</span> len(neighbors) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> len(neighbors) &gt; self.max_degree:</span><br><span class="line">            neighbors = np.random.choice(neighbors, self.max_degree, replace=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">elif</span> len(neighbors) &lt; self.max_degree:</span><br><span class="line">            neighbors = np.random.choice(neighbors, self.max_degree, replace=<span class="literal">True</span>)</span><br><span class="line">        adj[self.id2idx[nodeid], :] = neighbors</span><br><span class="line">    <span class="keyword">return</span> adj, deg</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UniformNeighborSampler</span><span class="params">(Layer)</span>:</span></span><br><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> Uniformly samples neighbors.</span></span><br><span class="line"><span class="string"> Assumes that adj lists are padded with random re-sampling</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">(self, adj_info, kwargs)</span>:</span></span><br><span class="line">     super(UniformNeighborSampler, self).init(kwargs)</span><br><span class="line">     self.adj_info = adj_info</span><br><span class="line">     </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">_call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">     ids, num_samples = inputs</span><br><span class="line">     adj_lists = tf.nn.embedding_lookup(self.adj_info, ids) </span><br><span class="line">     adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(adj_lists)))</span><br><span class="line">     adj_lists = tf.slice(adj_lists, [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">-1</span>, num_samples])</span><br><span class="line">     <span class="keyword">return</span> adj_lists</span><br></pre></td></tr></table></figure>
<p>在构建邻接表时，对度数小于max_degree的点采用了有重复采样，而对于度数超过max_degree的点才用了无重复采样。</p>
<p>接下来，在真正为每个batch内的目标点选取receptive field点时，代码在model.py中，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, inputs, layer_infos, batch_size=None)</span>:</span></span><br><span class="line">    <span class="string">""" Sample neighbors to be the supportive fields for multi-layer convolutions.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        inputs: batch inputs</span></span><br><span class="line"><span class="string">        batch_size: the number of inputs (different for batch inputs and negative samples).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> batch_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        batch_size = self.batch_size</span><br><span class="line">    samples = [inputs]</span><br><span class="line">    <span class="comment"># size of convolution support at each layer per node</span></span><br><span class="line">    support_size = <span class="number">1</span></span><br><span class="line">    support_sizes = [support_size]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(len(layer_infos)):</span><br><span class="line">        t = len(layer_infos) - k - <span class="number">1</span></span><br><span class="line">        support_size *= layer_infos[t].num_samples</span><br><span class="line">        sampler = layer_infos[t].neigh_sampler</span><br><span class="line">        node = sampler((samples[k], layer_infos[t].num_samples))</span><br><span class="line">        samples.append(tf.reshape(node, [support_size * batch_size,]))</span><br><span class="line">        support_sizes.append(support_size)</span><br><span class="line">    <span class="keyword">return</span> samples, support_sizes</span><br></pre></td></tr></table></figure>
<p>这里的采样过程与BFS类似，首先找到一个目标节点，之后是该节点的一阶邻居节点，之后迭代。</p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/03/19/Graph-Attention-Networks-笔记/" class="prev">PREV</a><a href="/2019/03/15/Semi-Supervised-Classification-with-Graph-Convolutional-Networks-笔记/" class="next">NEXT</a></div><div class="copyright"><p>© 2019 <a href="http://yoursite.com">Zidi Chen</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>