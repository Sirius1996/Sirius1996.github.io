<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Graph Attention Networks 笔记 · Hexo</title><meta name="description" content="Graph Attention Networks 笔记 - Zidi Chen"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Hexo"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="https://weibo.com/u/5519474336" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/Sirius1996" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/%E5%AD%90%E8%BF%AA-%E9%99%88-43010911a/" target="_blank" class="nav-list-link">LINKEDIN</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Graph Attention Networks 笔记</h1><div class="post-info">Mar 19, 2019</div><div class="post-content"><p>针对图结构数据，本文提出了一种GAT（graph attention networks）网络。该网络使用masked self-attention层解决了之前基于图卷积（或其近似）的模型所存在的问题。在GAT中，图中的每个节点可以根据邻节点的特征，为其分配不同的权值。GAT的另一个优点在于，无需使用预先构建好的图。因此，GAT可以解决一些基于谱的图神经网络中所具有的问题。实验证明，GAT模型可以有效地适用于（基于图的）归纳学习问题与转导学习问题。<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>针对图结构数据，本文提出了一种GAT（graph attention networks）网络。该网络使用masked self-attention层解决了之前基于图卷积（或其近似）的模型所存在的问题。在GAT中，图中的每个节点可以根据邻节点的特征，为其分配不同的权值。GAT的另一个优点在于，无需使用预先构建好的图。因此，GAT可以解决一些基于谱的图神经网络中所具有的问题。实验证明，GAT模型可以有效地适用于（基于图的）归纳学习问题与转导学习问题。</p>
<h2 id="GAT-架构"><a href="#GAT-架构" class="headerlink" title="GAT 架构"></a>GAT 架构</h2><h3 id="Graph-Attentional-Layer"><a href="#Graph-Attentional-Layer" class="headerlink" title="Graph Attentional Layer"></a>Graph Attentional Layer</h3><p>本文提出的attentional layer输入是一组节点特征，$h={\vec h_1,\vec h_2,…,\vec h_N}​$ , $h_i \in \mathbb R^F ​$,其中，N是节点个数，F是每个节点的特征数。该层产生一组新的节点特征，作为其输出，即：h’ = {$\vec h’_1, \vec h’_2,…,\vec h’_N​$},$\vec h’_i \in \mathbb R^{F’}​$。</p>
<p>为了得到充分表达能力，将输入特征转换为高层特征，至少我们需要一个可学习的线性转换。为了达到该目标，作为初始步骤，一个共享的线性转换，参数化为weight matrix, W, 应用到每一个节点上。然后在每一个节点上，进行self-attention — a shared attentional mechanism a： 计算attention coefficients。</p>
<p><img src="/2019/03/19/Graph-Attention-Networks-笔记/image-20190320110539962.png" alt="image-20190320110539962"></p>
<p>表明节点j的特征对节点i的重要性。其中，a是一个$\mathbb R^{F’} \times \mathbb R^{F’} \to \mathbb R $的映射，$W \in \mathbb R^{F’ \times F}$是一个权值矩阵（被所有$\vec h_i$共享）。一般来说，self-attention会讲注意力分配到图中所有节点上，这种做法会丢失结构信息。为解决这一问题，本文采用了一种masked attention的方式——仅将注意力分配到节点i的邻节点集上，即$j \in \mathcal N_i$(在本文中，节点i也是$\mathcal N_i​$的一部分)。</p>
<p><img src="/2019/03/19/Graph-Attention-Networks-笔记/image-20190320162325906.png" alt="image-20190320162325906"></p>
<p>在本文中，a使用单层的前馈神经网络实现。总的计算过程为：</p>
<p><img src="/2019/03/19/Graph-Attention-Networks-笔记/image-20190320162413327.png" alt="image-20190320162413327"></p>
<p>其中，$\cdot ^ T$表示换位，$||$表示级联操作。$\vec a^T \in \mathbb R^{2F’}$为前馈神经网络的参数，$\mathit{LeakyReLU}$为前馈神经网络的激活函数。此时可以得到$\vec h_i’$:</p>
<p><img src="/2019/03/19/Graph-Attention-Networks-笔记/image-20190320162938666.png" alt="image-20190320162938666"></p>
<p>为了提高模型的拟合能力，本文中引入了多抽头的self-attention，即同时使用多个$W^k$计算self-attention，然后将各个$W^k$计算得到的结果合并（链接或求和）：</p>
<p><img src="/2019/03/19/Graph-Attention-Networks-笔记/image-20190320163540975.png" alt="image-20190320163540975"></p>
<p>其中，$||$表示链接，$a^k_{ij}$是由第k个attention coefficients计算的归一化attention系数，$W_k$是对应的输入线性变换的权重矩阵。</p>
<p>一个graph attention layer的结构如下图所示：</p>
<p><img src="/2019/03/19/Graph-Attention-Networks-笔记/image-20190320161232352.png" alt="image-20190320161232352"></p>
<p>具体来说，就是graph attention layer首先根据输入的节点特征向量集，进行self-attention处理：</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文提出了一种基于self-attention的图模型。总的来说，GAT的特点主要有：</p>
<ul>
<li>与GCN类似，GAT是一种局部模型。因此，训练GAT模型无需了解整个图结构，只需要知道每个节点的邻节点即可。</li>
<li>GAT与GCN有着不同的节点更新方式。GAT使用的是self-attention为每个邻节点分配权重。</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2019/03/19/Inductive-Representation-Learning-on-Large-Graphs-笔记/" class="next">NEXT</a></div><div class="copyright"><p>© 2019 <a href="http://yoursite.com">Zidi Chen</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>